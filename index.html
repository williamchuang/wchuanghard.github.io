---
layout: default
title: "William Chuang"
description: "Graduate Student at University of Arizona"
---

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>William Chuang</title>
 
  <link rel="stylesheet" href="/css/main.css">
</head>
<body>


  <!-- Main Container -->
  <main class="container">
    
  
    <section class="blurb">
      <h1 class="title">William Chuang</h1>
      <p class="subtitle">Graduate Student, University of Arizona</p>

      <div class="profile-image">
        <!-- If you want the image at 90% width, do inline or via CSS -->
        <img 
          src="/files/me_at_Notre_Dame.jpg"
          alt="Cathédrale Notre-Dame de Paris—my faith!"
          style="width: 90%;"
        />
      </div>

      <p class="contact-info">
        <strong>E-mail:</strong> williamchuang@arizona.edu
      </p>
      <p class="contact-info">
        <strong>Pronouns:</strong> he/him
      </p>

      <hr class="divider" />

      <p class="content-text">
        I enjoy hiking and painting (including oil painting), and I am also deeply
        curious about neuroscience, psychology, and classical studies. I am 
        particularly fascinated by innovative strategies for designing and 
        coding neural network models—especially those that can recursively 
        design, implement, and train new, evolved networks based on a parent 
        model’s best configuration. In addition, I love exploring how, when, 
        and what we learn to accomplish a chosen mission along all possible 
        paths, ultimately working toward a comprehensive “user manual” for 
        the human mind.
      </p>

      <p class="content-text">
        I have curated a selection of notes and resources to support 
        preparation for qualifying exams. These materials reflect some of my 
        approaches to key topics and problem-solving strategies. They are 
        available for review in the following Google Drive folder:
        <br>
        <a
          href="https://drive.google.com/drive/folders/1HDanu8evZ7ytPe4lZSUTETJmK4WZ2R2T?usp=sharing"
          class="external-link"
        >Access my Qualifying Exam Notes</a>
      </p>

      <hr class="divider" />

      <p class="content-text">
        Additionally, here is my YouTube channel, where I plan to share worked-through math problems regularly:
        <a 
          href="https://www.youtube.com/channel/UCdN4ayC6Q53Zs0A4_aYlBAA"
          class="external-link"
        >@william_chuang</a>
      </p>

      <hr class="divider" />

      <p class="content-text">
        You can find some of my older math notes here:
        <br>
        <a
          href="https://drive.google.com/drive/folders/1u2csqSsWxGHkgSIGil3xRVwxsYxwY2jI?usp=share_link"
          class="external-link"
        >My old notes</a>
      </p>

      <hr class="divider" />

      <p class="content-text">
        <strong>More About Me Before 2015</strong>
        <br>
        <a 
          href="/files/till_2014.pdf"
          class="external-link"
        >Detailed Records Prior to 2014</a>
      </p>

      <hr class="divider" />

      <p class="content-text">
        <strong>The Genealogy of the Chuang Family (also spelled Cheung, Chong, etc.)</strong>
      </p>
      <p class="content-text">
        According to statistics from the U.S. Census Bureau, there are an 
        estimated 2,931 individuals in the United States with the surname Chuang, 
        ranking it 11,621st in prevalence, at 0.92 per 100,000 people. In East 
        Asia, the name Chuang remains relatively rare; it ranks 323rd in the Song 
        Dynasty’s “Hundred Surnames.” As a result, many individuals with the 
        surname Chuang are unfamiliar with their lineage beyond three generations 
        and may even feel like outsiders to the larger clan.
      </p>

      <p class="content-text">
        Historical data indicates that fewer than 50,000 people (around 0.06% of 
        the total population) bore the surname Chuang during the Song Dynasty, 
        with Fujian hosting the highest concentration. By the Ming Dynasty, 
        approximately 120,000 people (about 0.12% of the total population) 
        carried this surname. Today, the name is most commonly found in Guangdong, 
        Fujian, Taiwan, Jiangsu, Zhejiang, Shandong, Heilongjiang, Jilin, 
        Shanghai, and Liaoning.
      </p>

      <p class="content-text">
        My mother was born in Wucuo (now Erlun) in Yunlin County, and my father 
        was born in Erlin Township—once known as Gielem (a region known for deer) 
        to the Dutch. I was born in New Taipei City, Taiwan, in 1988. Based on my 
        grandparents’ family records, I am the 20th generation of the Chuang 
        family to settle in Taiwan after our ancestors first arrived in the 1600s, 
        following the Dutch occupation. This migration reminds me of the Mayflower 
        pilgrims who arrived in the Americas around the same time—my ancestors 
        similarly ventured forth in pursuit of freedom and opportunity.
      </p>

      <p class="content-text">
        A more recent notable figure in the Chuang family is Zhuang Yunkuan (also 
        known as Yunkuan Chuang), who served as both a Qing Dynasty and Republic 
        of China politician, as well as a Chinese calligrapher. He was a delegate 
        in drafting the Republic of China’s provisional constitution and, in 1925, 
        joined the board of directors of the National Palace Museum.
      </p>

      <p class="content-text">
        There is also a branch of the Chuang family in Guangdong and Hong Kong. 
        Among its most well-known members are Cheung Jing-on, his daughter 
        Chong Yuet-ming, and his nephew.
      </p>
    </section>

    <!-- Inserted Content: ALL Centered -->
    <section class="research-journey centered-content">
      <h2>Advancing Transformer Efficiency Through Dynamic Scaling Factors: My Research Journey</h2>

      <h3>Introduction</h3>
      <p>
        The transformer architecture has revolutionized deep learning, powering state-of-the-art large language models (LLMs) such as GPT-4. However, the reliance on brute computational power to scale these models presents significant challenges, including high costs and inefficiency. My research focuses on dynamically optimizing the scaling factor \(\beta\) in transformers to improve efficiency and accuracy. This journey has been both challenging and rewarding, and I am proud to share the progress I have made.
      </p>
      <hr class="divider">

      <h3>Timeline and Research Progress</h3>
      <div class="timeline">




<div class="timeline-item">
  <h4>Early Encounters with the Ising Model</h4>
  <ul>
    <li>
      In 2008, I implemented my first Ising model code in a computational physics course using Fortran 99, taught by Dr. Chi-Ning Chen at NDHU. This experience introduced me to computational techniques in statistical physics and laid the foundation for my later studies of the model.
    </li>
    <li>
      Later, in my graduate course <em>Statistical Mechanics II</em> at NTU, taught by Dr. Ning-Ning Pang, I had the opportunity to present my final project as an independent study in May 2012. In this presentation, I studied the known solutions of the Ising model as introduced in 
      <a href="https://books.google.com/books/about/Statistical_Mechanics.html?id=yJUyAgAACAAJ" target="_blank">T.D. Lee’s lecture notes (Statistical Mechanics)</a>. 
      After reading it, I found that these solutions might have a profound connection to the Riemann zeta function in number theory or complex analysis, which became the focus of my independent study.
    </li>
    <li>
      Reflecting on this work, I find Charles M. Newman's 2016 minicourse to be a particularly articulate exploration of the interplay between analytic number theory and statistical mechanics. While my presentation predated this minicourse, his insights provide a valuable modern perspective on these connections. The abstract of his lectures can be found 
      <a href="https://math.ecnu.edu.cn/gg/Math_Minicourse_by_Prof_Newman_20160223_to_20160322.pdf" target="_blank">here</a>, 
      and the full lectures are available on YouTube:<br>
      <ul>
        <li><a href="https://www.youtube.com/watch?v=H64g1N6sW0w" target="_blank">Lecture 1</a></li>
        <li><a href="https://www.youtube.com/watch?v=lF1bJj9pUjI" target="_blank">Lecture 2</a></li>
        <li><a href="https://www.youtube.com/watch?v=1P3aAYG21IY" target="_blank">Lecture 3</a></li>
        <li><a href="https://www.youtube.com/watch?v=f9y419oS544" target="_blank">Lecture 4</a></li>
        <li><a href="https://www.youtube.com/watch?v=mKPEntLn_wc" target="_blank">Lecture 5</a></li>
      </ul>
    </li>


    <li>
      Following this, I further explored the Ising model and its broader implications through various perspectives. I engaged with key references, including David Tong's 
      <a href="http://www.damtp.cam.ac.uk/user/tong/sft.html" target="_blank">lectures on Statistical Field Theory</a>, 
      Paul Ginsparg's <a href="https://arxiv.org/abs/hep-th/9108028" target="_blank">Applied Conformal Field Theory</a>, 
      and Kerson Huang's <a href="http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/102S112/1" target="_blank">Statistical Mechanics course at NTU</a>.
    </li>

    <li>
      Furthermore, I studied Landau's and Feynman's approaches to statistical mechanics, which provided deeper insights into the underlying mathematical structures. My independent study with Dr. Heng-Yu Chen at NTU further solidified my understanding, particularly in the context of field-theoretic methods and their applications to statistical physics.
    </li>


<li>
During my Intro to CS course at USF in 2015, I discussed with Dr. Cindi Thompson how the Ising model could be used to explain deep learning neural networks during her office hours. At that time, we also read and shared about three or four research papers on this topic.    

</li>
    <li>
      Additionally, after reviewing the online lectures of Chuck Newman, as recommended by Prof. Sunder Sethuraman, I worte three notes that further explore these connections in detail:
      <ul>
        <li><a href="/files/Introduction to Chuck Newman's Work on Statistical Mechanics and the Riemann Hypothesis.pdf" target="_blank">Introduction to Chuck Newman's Work on Statistical Mechanics and the Riemann Hypothesis</a></li>
        <li><a href="/files/Recognizing Self-Attention as a Stack of Ising Models- A Theoretical Perspective.pdf" target="_blank">Recognizing Self-Attention as a Stack of Ising Models: A Theoretical Perspective</a></li>
        <li><a href="/files/Unifying Ising Models, Prime Distribution, and Transformers.pdf" target="_blank">Unifying Ising Models, Prime Distribution, and Transformer Optimization via β Scaling</a></li>
      </ul>
    </li>

  </ul>
</div>


        <div class="timeline-item">
          <h4>December 2022 – January 2023</h4>
          <ul>
            <li>Began investigating the role of the scaling factor \(\beta\) in self-attention mechanisms.</li>
            <li>Developed theoretical foundations inspired by statistical mechanics and optimization theory to dynamically adjust \(\beta\).</li>
          </ul>
        </div>

        <div class="timeline-item">
          <h4>September 2023</h4>
          <ul>
            <li>Drafted the first version of my research paper, focusing on the theoretical basis and moderate empirical results to maintain credibility while avoiding overstatements.</li>
          </ul>
        </div>

        <div class="timeline-item">

        <div class="timeline-item">
          <h4>December 2023</h4>
          <ul>
            <li><strong>RTG Presentation</strong>: Presented a preliminary version of my work at the RTG seminar at the University of Arizona.
              <ul>
                <li>The presentation focused on moderate improvements in model performance by dynamically optimizing \(\beta\).</li>
                <li>Received mixed feedback, with some skepticism due to the lack of large-scale demonstrations.</li>
              </ul>
            </li>
          </ul>
        </div>

<div class="timeline-item">
  <h4>October 30, 2024</h4>
  <ul>
    <li><strong>Export Office Rejection</strong>:
      <ul>
        <li>Contacted the Export Control Office at the University of Arizona to ensure compliance with dual-use regulations.</li>
        <li>Despite explaining the potential dual-use nature of my work, the export office declined to classify it as significant or requiring clearance.</li>
        <li><strong>Their Response</strong>: "We do not need to clear your work on any of the projects you have described."</li>
        <li><strong>Impact</strong>: This rejection reflected a lack of institutional recognition of the potential importance of my work for U.S. competitiveness and national security.</li>
        <!-- Images Section -->
        <li class="images-section">
          <div class="image-container">
            <figure>
              <img src="/files/export_office.png" alt="Description of Transformer-Based LLM Training Efficiency" class="timeline-image">
              <figcaption>Portion of the description I wrote.</figcaption>
            </figure>
          </div>
          <div class="image-container">
            <figure>
              <img src="/files/export_office-reply.png" alt="Export Office Reply" class="timeline-image">
              <figcaption>Last email I received from the Export Control Office.</figcaption>
            </figure>
          </div>
        </li>
      </ul>
    </li>
  </ul>
</div>

        <div class="timeline-item">
          <h4>December 2024</h4>
          <ul>
            <li>Published the work on <strong>ResearchGate</strong> to ensure accessibility and transparency. While ResearchGate has a smaller reach than arXiv, it allowed me to share my results with the academic community.</li>
          </ul>
        </div>

        <div class="timeline-item">
          <h4>January 2025</h4>
          <ul>
            <li>Preparing further refinements to the paper, incorporating additional experimental results and practical implications to submit to alternative venues.</li>
          </ul>
        </div>
      </div>
      <hr class="divider">

      <h3>Key Contributions</h3>
      <ol>
        <li>
          <strong>Dynamic Scaling Factor Optimization</strong>:
          <ul>
            <li>Proposed a dynamic adjustment to the traditional scaling factor (\(\beta = \frac{1}{\sqrt{d_k}}\)) used in transformers.</li>
            <li>Demonstrated that a dynamically optimized \(\beta\) significantly improves test accuracy across various datasets and model configurations.</li>
            <li>Published moderate results showing substantial improvements over traditional methods without overstating claims.</li>
          </ul>
        </li>
        <li>
          <strong>Experimental Results</strong>:
          <ul>
            <li>The results showcase consistent improvements in accuracy when using the dynamic scaling factor compared to the traditional fixed method.</li>
            <li>Key findings include accuracy improvements across varying categories, sequence lengths, and training set sizes.</li>
          </ul>
        </li>
        <li>
          <strong>Theoretical Foundation</strong>:
          <ul>
            <li>Derived the dynamic scaling factor optimization method based on insights from statistical mechanics and energy minimization principles.</li>
            <li>Demonstrated the theoretical soundness of the method in reducing redundancy and enhancing efficiency in self-attention mechanisms.</li>
          </ul>
        </li>
      </ol>
      <hr class="divider">

<section id="landau-preface">
  <h2>Landau’s 1940 Preface</h2>

  <!-- English Translation Only -->
  <div class="english-text">
    <p><strong>Theoretical Physics Course · Mechanics</strong></p>
    <p>
      As everyone knows, physics consists of two main disciplines: experimental physics and theoretical physics. The large number of physical laws we know can be derived from a small number of very general principles. Such derivation, and the establishment of those general principles, call for a distinctive method, and this method defines a particular branch of study—namely, theoretical physics.
    </p>
    <p>
      Theoretical physics uses mathematical tools and methods to arrive at its own results and conclusions. However, theoretical physics differs fundamentally from mathematics in that it has a direct link to experimental results. This is not to suggest that the most general laws can only be built on experimental data, nor that drawing conclusions from those laws does not also require prior experimental investigations. Without such investigations, one cannot judge which among the many interwoven factors are important or negligible. Once the relative importance of these factors is known, the essential task of theoretical physics is essentially complete. Further application of these equations to specific cases of varying complexity soon becomes a matter of purely mathematical study, forming what we call “mathematical physics.”
    </p>
    <p>
      The goal of theoretical physics is to establish physical laws, that is, to establish relationships among physical quantities. Determining the specific numerical values of those quantities is generally not the task of theoretical physics, since, for numerical issues, experimental methods are often simpler and do not require labor-intensive calculations. Naturally, if a situation is simple enough, theory can directly compute the numerical values.
    </p>
    <p>
      It must be emphasized that theoretical physics aims to establish and characterize the relationships between the physical quantities of a given phenomenon. Consequently, one can only devise a proper theory if such relationships truly exist in nature. Yet in many cases, the physical quantities of interest bear no relation to each other at all; in other words, they belong to entirely separate categories in different natural phenomena. Hence, in certain situations, the absence of a dedicated theory does not imply an inability to explain that phenomenon; if the most general laws can yield the same result, there is no necessity for a specialized theory.
    </p>
    <p>
      Approximate analysis plays a tremendous role in theoretical physics. First, every “exact” law is in reality approximate, because in the vast majority of cases, that approximation offers sufficient accuracy. Second, theoretical physics does not strictly demand absolute accuracy in physical laws. If one defines the scope of a given phenomenon in advance, it suffices for the outcome to meet the required degree of precision. That is why we can still use Newtonian mechanics for analyzing the trajectory of artillery shells, despite knowing it is not absolutely accurate, simply because it is sufficiently precise in that domain, and we turn to relativity only when necessary for higher accuracy.
    </p>
    <p>
      For this reason, in theoretical physics, there coexist certain theories (often referred to as “classical theories”) that have been shown to be less accurate alongside those that are more exact. They remain useful because, within certain specific ranges of phenomena, they retain their applicability. Any logically complete theory, once verified as valid within a certain accuracy range, does not lose its value. Indeed, partial or approximate results, derived in particular cases, remain embedded in any subsequent, more precise theory. Plainly, this category also includes those still under development or not yet fully coherent; they, too, have significance in the progression of theoretical physics.
    </p>
    <p>
      Thus, we see that a key process in general physical theory lies in deducing more specific laws from the most general principles, without neglecting the central role of careful consideration of the most important factors. Overlooking those primary factors while relying solely on coarse simplifications can lead to ignoring the true scale or magnitude of the phenomena. In reality, the forms of phenomena themselves are often approximate, and the functional relationships among the physical quantities that describe them are similarly approximations. When studied at higher levels of precision, these relationships may reveal deeper meanings.
    </p>
    <p>
      Determining the level of approximation at which one examines a phenomenon is exceptionally important in theoretical research. <strong>The gravest error is to adopt an extremely precise theory and exhaustively compute every subtle correction, while failing to recognize the broader advantages that a more streamlined or holistic approach might offer.</strong>
    </p>
    <p>
      <strong>L. D. Landau<br>
      1940</strong>
    </p>
    <p>
      <small>(Note: Landau wrote this preface in 1940, when computational tools were very limited, so numerical experiments remained challenging.)</small>
    </p>
  </div>
</section>

<section id="landau-connection">
  <h2>Relevance of Landau’s 1940 Preface to My Research</h2>
  <p>
  I find Landau’s perspective in his <em>1940 Preface to Theoretical Physics Course</em> particularly resonant with the challenges in large-scale machine learning today. My academic path, spanning mathematics, physics, and computer science, allows me to appreciate how Landau’s emphasis on identifying key parameters and simplifying complex systems parallels the efficient training of transformer architectures. His insight—that theory provides a guiding framework but requires the isolation and rigorous examination of the most critical factors to achieve practical, approximate solutions—is especially relevant to machine learning, where computational resources are finite and model complexity can be immense.
  </p>

 <p>
    Specifically, Landau’s discussion about leveraging general principles to sift out essential elements is deeply relevant to
    the “scaling factor,” or “temperature parameter,” often denoted by <strong>β</strong>, in transformer-based self-attention.
    Much like Landau’s insistence on identifying the key parameters governing physical phenomena, a dynamically optimized β
    pinpoints the core drivers of attention mechanism performance. Rather than devoting overwhelming computational effort to
    brute-force hyperparameter tuning, the principle of focusing on the most significant contributing factors—echoing Landau’s
    approach—yields both conceptual clarity and practical efficiency in modern AI models.
  </p>

  <p>
  In the context of transformers, the traditional scaling factor \( \beta = \frac{1}{\sqrt{d_k}} \), introduced in <em>Attention is All You Need</em>, is treated as a fundamental parameter for ensuring stable self-attention dynamics. However, Landau’s perspective challenges us to question whether such heuristics truly reflect the underlying physics or mathematics of the system. If we consider the established equivalence between deep neural networks and spin-glass models, as demonstrated in LeCun’s seminal work on loss landscapes, the role of \( \beta \) becomes analogous to the inverse temperature in the Ising model—a parameter deeply tied to criticality and phase transitions. <strong>Could it be that this choice of \( \beta \) oversimplifies the dynamics of transformers and N-dim Ising models, ignoring subtleties that a more rigorous, theoretically grounded approach might uncover?</strong>
  </p>
  <p>
  By leveraging the mathematical connections between Ising models, statistical mechanics, and deep learning, I argue that a dynamic optimization of \( \beta \), informed by principles from energy minimization and criticality, offers a pathway to more efficient and scalable transformer architectures. This approach not only aligns with Landau’s methodological rigor but also holds the potential to address long-standing challenges in both machine learning and statistical physics, such as solving N-dimensional Ising-like problems. I invite the broader academic and machine learning communities to explore these connections further, using well-established mathematics to refine hyperparameter selection and advance the field.
  </p>

  <p>   
    Finally, in the same way Landau accentuates the intimate relationship between theoretical foundations and experimental
    verification, my research underscores that the best outcomes come from bridging foundational theory with empirical tuning.
    I capitalize on the dynamic nature of \( \beta \)—rooted in statistical mechanics and energy minimization—to guide real-time updates
    of the self-attention process. This holistic cycle of theory informing practice, and vice versa, illustrates precisely why
    Landau’s arguments still hold tremendous value today: when major parameters are systematically refined based on a sound
    theoretical framework, significant leaps in performance and efficiency can be realized.
  </p>
</section>

<section id="ising-deep-learning">
  <h2>Connecting the Ising Model to Deep Learning and Transformers</h2>

  <p>
    The mathematical and theoretical connections between the Ising model, spin-glass systems, and modern deep learning architectures like transformers have been well-studied. The following notable works highlight these connections, providing a foundation for understanding the equivalence or similarity between these systems:
  </p>

  <h3>Key Papers and Abstracts</h3>
  <ol>
    <li>
      <strong>"The Loss Surfaces of Multilayer Networks" (2015)</strong>  
      <em>Authors:</em> Anna Choromanska, Mikael Henaff, Yann LeCun, et al.  
      <p>
        This foundational paper investigates the landscape of loss surfaces in deep neural networks, using tools from statistical physics. The authors demonstrate that the structure of loss surfaces in multilayer networks can be analyzed through connections to the energy landscapes of spin-glass models, such as the Ising model. This work establishes theoretical parallels between deep learning and statistical mechanics, providing insights into why neural networks are able to find good minima despite the complexity of their loss surfaces.
      </p>
      <a href="http://proceedings.mlr.press/v38/choromanska15.pdf" target="_blank">Read the Paper</a>
    </li>
    <li>
      <strong>"Deep Learning the Ising Model Near Criticality" (2017)</strong>  
      <em>Authors:</em> Alan Morningstar and Roger G. Melko  
      <p>
        This study investigates the capability of deep generative models, such as Deep Boltzmann Machines and Deep Belief Networks, to learn the probability distribution of a two-dimensional Ising system. The authors compare these deep architectures to shallow networks like Restricted Boltzmann Machines, focusing on their accuracy in generating energetic observables near the phase transition.
      </p>
      <a href="https://arxiv.org/abs/1708.04622" target="_blank">Read the Paper</a>
    </li>
    <li>
      <strong>"Explaining the Machine Learning Solution of the Ising Model" (2023)</strong>  
      <p>
        This paper shows how a neural network without hidden layers can determine the critical temperature of the ferromagnetic Ising model's phase transition. The study provides insights into the strategies employed by neural networks in solving such problems, paving the way for explainable machine learning applications in physics.
      </p>
      <a href="https://arxiv.org/abs/2402.11701" target="_blank">Read the Paper</a>
    </li>
    <li>
      <strong>"Ising Models of Deep Neural Networks" (2022)</strong>  
      <em>Authors:</em> Dusan Stosic, Darko Stosic, Borko Stosic  
      <p>
        The authors map deep neural networks to classical Ising spin models, allowing for a description using statistical thermodynamics. The study reveals that well-trained networks exhibit structures in their weights that span a wider range of realizable energies compared to poorly trained ones.
      </p>
      <a href="https://arxiv.org/abs/2209.08678" target="_blank">Read the Paper</a>
    </li>
    <li>
      <strong>"Inverse Ising Inference by Combining Ornstein-Zernike Theory with Deep Learning" (2017)</strong>  
      <p>
        This research establishes an analogy between the inverse Ising problem and the Ornstein-Zernike formalism in liquid state physics. A deep neural network is employed to learn closure relations from Ising model simulations, outperforming traditional methods in inferring generative models from data.
      </p>
      <a href="https://arxiv.org/abs/1706.08466" target="_blank">Read the Paper</a>
    </li>
    <li>
      <strong>"A Deep Dive into the Connections Between the Renormalization Group and Deep Learning in the Ising Model" (2023)</strong>  
      <em>Author:</em> Kelsie Taylor  
      <p>
        This paper examines parallels between unsupervised deep learning and renormalization group flow through the lens of the two-dimensional Ising model. Restricted Boltzmann Machines are used to explore whether deep learning can be interpreted as a layer-by-layer coarse-graining process akin to renormalization.
      </p>
      <a href="https://arxiv.org/abs/2308.11075" target="_blank">Read the Paper</a>
    </li>
  </ol>

  <h3>Questioning the Validity of Traditional β in Transformers</h3>
  <p>
    Inspired by the works above, particularly LeCun's analysis of loss landscapes in deep networks, consider this reasoning:
  </p>
  <ol>
    <li>
      Transformers, like deep neural networks, share mathematical similarities with Ising models or spin-glass systems, as evidenced by LeCun's work and others.
    </li>
    <li>
      The traditional scaling factor \( \beta = \frac{1}{\sqrt{d_k}} \), used in transformer self-attention mechanisms (introduced in <em>"Attention is All You Need"</em>), determines the variance of attention scores. This is mathematically analogous to the inverse temperature \( \beta \) in the Ising model, which governs the phase transitions of spin systems.
    </li>
    <li>
      If the traditional method of choosing \( \beta \) (introduced in <em>"Attention is All You Need"</em>) is correct and optimal, it would imply that the authors of <em>"Attention is All You Need"</em> have effectively solved the N-dimensional Ising model by identifying its critical temperature through the standard deviation of all weights (i.e., the components of vectors in a spin-glass system).
    </li>
  </ol>

  <p>
    <strong>But could this really be true?</strong> If the N-dimensional Ising model, a longstanding open problem in statistical mechanics, has not been solved, then does this suggest the traditional scaling factor \( \beta = \frac{1}{\sqrt{d_k}} \) might be suboptimal or oversimplified? Could dynamic optimization of β, as proposed in my work, offer a better approximation for the true critical behavior of these systems?
  </p>

<p>
  This reasoning employs <strong>proof by contradiction</strong>. By challenging the solution proposed in <em>Attention is All You Need</em> using the standard deviation of weights, i.e., parameters, for determining <strong>\( \beta \)</strong>, and examining its equivalence to the critical temperature in the Ising model, I question whether the traditional scaling factor truly captures the underlying dynamics of transformer architectures and provides the solution to N-dimensional Ising-like models. 
  I invite the community of academia, LLMs, and machine learning to further study this connection/similarity/equivalence and use it—and the well-established mathematics behind it—to determine the hyperparameters of LLMs.
</p>


</section>

<section class="supporting-visuals">
  <h3>Supporting Visuals</h3>
  
  <figure>
    <img src="/files/Nouri-1.png" alt="Screenshot of Steve Nouri's LinkedIn post discussing transformer architectures optimization" class="timeline-image">
    <figcaption>Figure 1: Steve Nouri's LinkedIn post.</figcaption>
  </figure>
  
  <figure>
    <img src="/files/Nouri-2.png" alt="Screenshot of Steve Nouri's LinkedIn post about AI efficiency advancements" class="timeline-image">
    <figcaption>Figure 2: Steve Nouri's LinkedIn post.</figcaption>
  </figure>
  
  <figure>
    <img src="/files/heat-map-2025-01-27.png" alt="Heat map of the U.S. stock market on January 27, 2025, showing significant market evaporation" class="timeline-image">
    <figcaption>Figure 3: Heat map of the U.S. stock market on January 27, 2025, illustrating a $2 trillion market evaporation.</figcaption>
  </figure>
</section>


      <h3>How My Research Could Have Prevented the $2 Trillion Market Evaporation</h3>
      <p><strong>The Problem:</strong></p>
      <p>
        On January 27, 2025, U.S. markets lost $2 trillion in value due to the emergence of cost-efficient AI models like DeepSeek R1, developed by a Chinese AI startup. DeepSeek claimed to have built a large language model rivaling GPT-4 at a fraction of the cost, leveraging extreme efficiency and open-source access. This disruption highlighted vulnerabilities in the U.S. AI ecosystem, which relies heavily on brute computational power and proprietary, high-cost approaches.
      </p>


      <p><strong>How My Work Could Have Helped:</strong></p>
      <ol>
        <li>
          <strong>Cost-Efficiency Leadership:</strong>
          <ul>
            <li>My research on dynamically optimizing the scaling factor \(\beta\) addresses the inefficiencies that made U.S. companies vulnerable to DeepSeek's disruption.</li>
            <li>By significantly improving transformer efficiency, my approach could have reduced the training costs of models like GPT-4, allowing U.S. companies to compete on cost with DeepSeek.</li>
          </ul>
        </li>
        <li>
          <strong>Early Adoption by U.S. Companies:</strong>
          <ul>
            <li>If my work had been published on arXiv earlier and adopted by companies like NVIDIA or OpenAI, they could have implemented cost-saving measures to preempt DeepSeek's advantage.</li>
            <li>This would have demonstrated U.S. leadership in AI efficiency, reducing the market shock caused by DeepSeek's announcement.</li>
          </ul>
        </li>
        <li>
          <strong>Strengthening U.S. AI Leadership:</strong>
          <ul>
            <li>My work offers a paradigm shift from brute computational scaling to intelligent optimization, positioning U.S. companies to lead the AI industry in both performance and cost-efficiency.</li>
            <li>Early integration of my methods into U.S. models would have preserved investor confidence in U.S. tech companies, preventing the massive market correction.</li>
          </ul>
        </li>
        <li>
          <strong>National and Economic Security:</strong>
          <ul>
            <li>My results could have been framed as part of a broader strategy to secure U.S. leadership in AI, reducing the risk of economic shocks caused by foreign competition.</li>
            <li>By highlighting the strategic importance of efficiency-focused AI research, my work could have attracted government and industry support, ensuring its timely implementation.</li>
          </ul>
        </li>
      </ol>
      <hr class="divider">

      <h3>Published Results</h3>
      <p>
        The key results from my work are summarized in the following table:
      </p>

      <div class="table-responsive">
        <table class="results-table">
          <thead>
            <tr>
              <th>Scaling Factor</th>
              <th>Accuracy (%)</th>
              <th>Observations</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>\(\beta = \frac{1}{\sqrt{d_k}}\) <br/> ~0.25</td>
              <td>1.50</td>
              <td>Traditional method</td>
            </tr>
            <tr>
              <td>\(\beta_{\text{opt}} = 6.67\)</td>
              <td>96.48</td>
              <td>Dynamic method</td>
            </tr>
          </tbody>
        </table>
      </div>
      <hr class="divider" />
    </section>
<h3>Future Directions</h3>
<p>
    Future work should extend this approach to diverse, real-world tasks, such as language modeling, machine translation, and computer vision. Second, the n-nary search algorithm, though effective, introduces computational overhead, which may be prohibitive for large-scale models. Developing more efficient algorithms for dynamically estimating \(\beta_{\text{opt}}\) is an important area for future research. Lastly, the interaction between \(\beta\) and other critical hyperparameters, such as learning rate, weight initialization, and attention head configurations, remains underexplored. A more comprehensive understanding of these interactions could lead to more holistic approaches to hyperparameter optimization.
</p>

<p>
    My research from September 2023 demonstrated the necessity of examining all other hyperparameters to uncover potential redundancies in transformer architectures. By employing concepts from the Ising model (spin-lattice, i.e., quantized), it becomes possible to compress or distill models further by quantizing parameter values. Parameters that are zeroed out through this approach reveal redundancy within the model. This insight suggests that the quantization process, informed by spin-lattice physics, could be a key tool for improving model efficiency and scalability.
</p>

<p>
    Furthermore, the algorithm I proposed for dynamically estimating \(\beta_{\text{opt}}\) can be translated into a reinforcement training framework. In this context, the algorithm adapts \(\beta\) in response to model feedback, effectively treating the optimization of \(\beta\) as a reinforcement learning problem. By iteratively refining \(\beta_{\text{opt}}\) based on real-time performance metrics, the algorithm could enable more adaptive, task-specific training. This approach not only enhances model efficiency but also provides a pathway for integrating reinforcement training principles into hyperparameter optimization.
</p>

<section class="research-implications">
  <h2>Theoretical and Strategic Implications of My Research</h2>

  <h3>1. Challenging the Mainstream Perspective: "Attention is All You Need"</h3>
  <p>
    The widespread adoption of the scaling factor \( \beta = \frac{1}{\sqrt{d_k}} \) in transformer-based architectures, as introduced by Vaswani et al. (2017) in "Attention is All You Need," lacks rigorous theoretical justification in modern large-scale implementations. This heuristic, though convenient, assumes that the variance of the attention score matrix remains constant during training—a simplification that breaks down as models scale and undergo dynamic updates. 
  </p>
  <p>
    Insisting on this traditional approach without revisiting its mathematical foundation essentially encourages researchers and industry leaders to take <strong>shortcuts</strong>—not in the sense of reducing training cost or improving theoretical development, but in <strong>prioritizing rapid implementation and commercialization over fundamental scientific rigor</strong>. By rushing to deploy models and bring AI products to market, mainstream researchers neglect deeper structural insights, leading to suboptimal architectures that persist due to inertia in the field.
  </p>
  <p>
    The implications are profound: adhering to outdated methods risks stagnation in the field, perpetuating inefficiencies and hindering progress. My research not only addresses this gap but also paves the way for a principled re-evaluation of foundational assumptions in transformer architectures.
  </p>

  <h3>2. Strategic Implications of DeepSeek's Efficiency</h3>
  <p>
    The emergence of cost-efficient AI models, such as DeepSeek R1 developed by a Chinese startup, underscores the strategic vulnerabilities inherent in the U.S. AI ecosystem. By leveraging extreme efficiency, DeepSeek has demonstrated the capability to rival state-of-the-art models like GPT-4 at a fraction of the cost. This development is not merely a technological achievement but a potential game-changer in geopolitics and economic security.
  </p>
  <p>
    If the PRC employs such advancements to iteratively improve their AI models and subsequently use those models to optimize semiconductor design and manufacturing, the implications are alarming. The current restrictions on NVIDIA, TSMC, and other chip manufacturers would become significantly less effective. Enhanced AI models could drive innovations in chip architecture, fabrication techniques, and supply chain efficiency, enabling the PRC to circumvent traditional dependencies on U.S. and allied technologies.
  </p>
  <p>
    My research offers a counter-strategy by emphasizing efficiency and cost reduction in transformer architectures. By adopting dynamic scaling methods, U.S. companies can maintain a competitive edge, mitigating the risk of market disruptions caused by foreign competitors. Furthermore, integrating my methods into U.S. AI and semiconductor strategies would reinforce national security, ensuring leadership in critical technologies.
  </p>

  <h3>3. Strategic Countermeasures: Ensuring U.S. AI and Semiconductor Superiority</h3>
  <p>
    To counter the potential developments enabled by PRC’s AI-driven semiconductor advancements, proactive solutions must be implemented. Below are two publicly discussable strategies that can be disclosed without reliance on compartmentalized measures, ensuring operational security while mitigating the risk of unauthorized interception:
  </p>

  <h4>Solution 1: Establishing AI-Empowered Semiconductor R&D Initiatives</h4>
  <p>
    The U.S. must invest heavily in AI-accelerated semiconductor research to preemptively outpace adversarial developments. This includes:
    <ul>
      <li>Integrating AI-driven chip design optimization using dynamic scaling factors to enhance efficiency and reduce costs.</li>
      <li>Funding collaborative research efforts between national labs, defense contractors, and academic institutions to develop secure, energy-efficient AI chips.</li>
      <li>Leveraging my research on transformer scaling factors to fine-tune AI-based chip architecture for superior performance.</li>
    </ul>
  </p>

  <h4>Solution 2: AI-Optimized Export Controls and Supply Chain Resilience</h4>
  <p>
    Traditional export control mechanisms are becoming less effective in a world where AI can independently design optimized chips. Instead, the U.S. must:
    <ul>
      <li>Implement AI-driven monitoring systems to track semiconductor supply chains and detect unauthorized technology transfers.</li>
      <li>Develop reinforcement-learning-based dynamic trade policies that adapt in real-time to counteract PRC's AI-driven semiconductor progress.</li>
      <li>Ensure that any AI model used in high-security domains incorporates dynamic security protocols to prevent adversarial exploitation.</li>
    </ul>
  </p>

  <p>
    These countermeasures are essential to ensuring that U.S. advancements in AI and semiconductor technologies remain strategically dominant. By integrating adaptive scaling techniques into AI-driven semiconductor R&D and export control strategies, the U.S. can effectively neutralize emerging threats and maintain technological superiority.
  </p>
</section>
<section class="distinguishing-approach">
  <h3>Distinguishing My Approach from OpenAI and DeepSeek</h3>
  
  <p>
    While organizations such as OpenAI and emerging competitors like DeepSeek have made significant strides in developing large language models (LLMs), their methodologies fundamentally differ from my research. My approach is rooted in a rigorous theoretical framework that identifies intrinsic values and redundancies within model parameters and hyperparameters. This foundational understanding not only explains why DeepSeek's distillation methods can achieve impressive results but also offers a more efficient and principled pathway to training highly capable models.
  </p>
  
  <p>
    DeepSeek's method leverages the presence of redundant parameters—a phenomenon my research has elucidated as stemming from intrinsic values determined by the model's topology. By recognizing that a substantial portion of parameters do not contribute meaningfully to the model's performance, DeepSeek effectively reduces model complexity through distillation. However, this approach requires starting with a pre-trained model, which inherently involves significant computational resources and time. Moreover, distillation methods may inadvertently raise concerns about intellectual property, as they often necessitate access to the trained model's internal parameters.
  </p>
  
  <p>
    In contrast, my research introduces a dynamic optimization framework that proactively identifies and eliminates these redundancies during the early stages of training—typically within the first few epochs. By optimizing hyperparameters such as the scaling factor \(\beta\) based on the model's topology, my approach ensures that only the most essential parameters are retained from the outset. This method drastically reduces the need for extensive training data and computational power. Empirical evidence from my studies demonstrates that optimizing a single hyperparameter can uncover significant redundancies, achieving accuracies ranging from approximately 1% to 97-100% using only 20% or less of the training data.
  </p>
  
  <p>
    Furthermore, my approach addresses a critical limitation in mainstream methods, including those employed by OpenAI's ChatGPT, which typically operate under the assumption that intrinsic hyperparameter values are unknown or that dynamic optimization techniques to approximate these values are not feasible. This leads to a reliance on exhaustive hyperparameter searches and large-scale data requirements, which are both time-consuming and resource-intensive.
  </p>
  
  <p>
    By leveraging a theoretically grounded method to ascertain intrinsic hyperparameter values dynamically, my approach eliminates the need for post-training distillation. This not only streamlines the training process but also avoids potential legal and ethical issues related to model distillation from proprietary APIs like OpenAI's. Additionally, optimizing all hyperparameters early on and dynamically zeroing out redundant parameters (akin to quantization with specific value gaps) paves the way for training models with significantly fewer parameters and less data, without compromising—and indeed enhancing—model performance.
  </p>
  
  <p>
    The intrinsic value of hyperparameters such as \(\beta\) is predominantly determined by the model's topology, including factors like the number of connections each node possesses. This contrasts sharply with DeepSeek's distillation methods, which lack insight into these intrinsic values and thus rely on heuristics and post-hoc adjustments. My method provides a principled way to determine these values from the outset, ensuring that the model is both efficient and effective without the need for later modifications.
  </p>
  
  <p>
    Moreover, if optimizing a single hyperparameter can reveal and eliminate large redundancies, it is reasonable to assert that a comprehensive optimization of all hyperparameters in the early stages of training could yield even more substantial improvements. Such an approach would enable models to achieve near-perfect accuracy with significantly less data by systematically identifying and zeroing out redundant parameters. This represents a paradigm shift in AI model training, moving towards more intelligent and efficient processes that are both data and computation-efficient.
  </p>
  
  <p>
    In summary, my research not only provides a deeper theoretical understanding of parameter and hyperparameter redundancies but also offers practical solutions that surpass existing methods. By dynamically optimizing hyperparameters based on model topology early in the training process, my approach ensures that models are both highly efficient and capable, setting a new standard in the field of machine learning.
  </p>
</section>
  
</main>

  <!-- Footer -->
  <footer>
    <ul class="footer-links">
      <!-- <li><a href="mailto:whchuang@usfca.edu">email</a></li> -->
      <li>
        <a href="https://github.com/williamchuang"
          >https://github.com/williamchuang</a
        >
      </li>
    </ul>
  </footer>

</body>
</html>

